#' translate an md file via LLM
#' 
#' `translate_llm()` will split the markdown file into chunks of around 100 lines
#' and make the llm proceed it through the instruction
#' 
#' @param md  md file name
#' @param instruction the text of the instructions to process the md file
#' @param url. the ollama REST API url 
#' 
#' @examples
#' parsed <- lightparser::split_to_tbl("my.qmd")
#' parsed |>
#'   unnest(cols = text) |>
#'   mutate(text_es = map(text,translate))
#' @export
#' @importFrom dplyr mutate lag lead pull filter slice_tail
#' @importFrom purrr map_dbl map2_chr
translate_llm <- function(md, instruction = instruction(from = "English", to = "French"), 
                          url. = "http://localhost:11434/api/generate"){
  # lightparse file & accumulate section size
  md_lp <- lightparser::split_to_tbl(md) |>
    mutate(nline = map_dbl(text, length) + map_dbl(code, length) -1,
                           n_section = cumsum(!is.na(heading)),
                           cum_line = cumsum(nline)
  ) |> tibble::rowid_to_column()
  
  # split into chunks
  ideal_lines <- 100L
  chunk_start <- c(1L)
  for (chunk_id in seq_len(max(md_lp$cum_line) %/% ideal_lines)) {
    prev_cum_line <- md_lp |> filter(rowid == chunk_start[chunk_id]) |> pull(cum_line)
    chunk_start[chunk_id + 1] <- md_lp |>
      filter(!is.na(heading_level), cum_line <= ideal_lines + prev_cum_line ) |>
      slice_tail(n = 1) |>
      pull(rowid)
    
    if (chunk_start[chunk_id + 1] == chunk_start[chunk_id]) {
      # next chink is too large for ideal_lines
      # we take next heading candidate
      chunk_start[chunk_id + 1] <- md_lp |>
        filter(!is.na(heading_level), cum_line > prev_cum_line ) |>
        slice_head(n = 1) |>
        pull(rowid)
    }
    
  }

  # md combine tbl back into chunks
  md_reassemble <- map2_chr(
    chunk_start,
    lead(chunk_start, default =  max(md_lp$rowid)) - 1,
    ~ lightparser::combine_tbl_to_file(md_lp[.x:.y, ])
  )
  
  # translate each chunk
  # md_translated <- furrr::future_map(
  #   md_reassemble,
  #   ~ ollama_api_get(model = "llama3.1:latest", prompt = paste0(instruction, .x), url = url.),
  #   .progress = TRUE
  # )
  md_translated <- purrr::map_chr(
    md_reassemble,
    ~ ollama_api_get(model = "llama3.1:latest", model_prompt = paste0(instruction, .x), url = url.),
    .progress = list(name = "Translation with ollama model:")
  )
  # combine result into a single md file
  md_translated
  
  }

#' create translation instructions
#'
#' @param from plain text language to translate from
#' @param to plain text language to translate into
#'
#' @return the translation instructions
#' @export
#' @examples
#' paste0(instruction(from="English", to = "French"), "Let's have a look at mtcars\n```{r}\nsummary(mtcars)\n```")
instruction <- function(from, to) {
  paste0("Please translate the ",from," text of the following rmarkdown vignette into ",to,
         ". Do not translate nor modify the YAML header delimited by `---`. ",
         "Do not translate nor modify the code chunks, but only the code comments. ",
         "Do not try to summarize your though or propose anything else, just provide the translated text.\n",
         "Rmarkdown vignette:\n\n")
}


#' translate text through ollama API
#'
#' @param model the ollama model name to call
#' @param instruction the ollama instruction
#' @param url the ollama API endpoint url
#'
#' @return text generated by the model
#' @export
#'
#' @examples
#' ollama_api_get(model = "llama3.1:latest", 
#'               model_prompt = "Why is the moon round ? provide a short answer")
#' @importFrom httr2 request req_body_raw req_perform resp_body_json
ollama_api_get <- function(model, model_prompt, url="http://localhost:11434/api/generate") {
  # the httr2 way
  body <-  paste0('{  "model": "',model,'",  "prompt": "',URLencode(model_prompt),'",  "stream": false}')
  # body <-  paste0("model=", model, "&prompt=", model_prompt, "stream=false")
  resp <- request(url) |>
    req_body_raw(body = body, type = "application/x-www-form-urlencoded") |>
    # req_body_form(model = model,
    #              prompt = model_prompt,
    #              stream = FALSE,
    #              type = "application/x-www-form-urlencoded") |>
    req_perform() |> 
    resp_body_json(simplifyVector = T)
  resp$response
}


